%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CS671: Machine Learning
% Copyright 2015 Pejman Ghorbanzade <mail@ghorbanzade.com>
% Creative Commons Attribution-ShareAlike 4.0 International License
% More info: https://bitbucket.org/ghorbanzade/umb-cs671-2015s
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Question 5}

Consider the following optimization problem.

\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}} & & x_1^2 + 2x_2^2\\
& \text{subject to} & & x_1 + x_2 - 1 \leqslant 0.\\
\end{aligned}
\label{eq51}
\end{equation}

Find a point satisfying the Kuhn-Tucker conditions.
Is that point an optimal solution?

\subsection*{Solution}

Let $\mathbf{x}(x_1, x_2)$ be a feasible solution of the optimization problem given in Eq. \ref{eq51}.
We define $f(\mathbf{x}) = x_1^2 + 2x_2^2$ and $g_1(\mathbf{x}) = x_1 + x_2 - 1$.
To verify Kuhn-Tucker conditions for any feasible solution $\mathbf{x}$, we first need to show that both functions $f$ and $g_1$ are convex and differentiable at $\mathbf{x}$.
Using the convexity theorem, $f$ is convex on $\mathbb{R}^2$ if and only if the Hessian matrix $H_f(\mathbf{x})$ is positive semidefinite for every $x\in \mathbb{R}^2$.
Hessian matrices $H_f(\mathbf{x})$ and $H_{g_1}(\mathbf{x})$ are computed in Eq. \ref{eq52} and Eq. \ref{eq53}, respectively.

\begin{equation}
H_f(\mathbf{x})
= \begin{pmatrix}
\frac{\partial^2 f}{\partial^2 x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2}\\[0.6em]
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial^2 x_2}\\
\end{pmatrix}
= \begin{pmatrix}
2 & 0\\[0.6em]
0 & 4\\
\end{pmatrix}
\label{eq52}
\end{equation}

\begin{equation}
H_{g_1}(\mathbf{x})
= \begin{pmatrix}
\frac{\partial^2 g_1}{\partial^2 x_1} & \frac{\partial^2 g_1}{\partial x_1 \partial x_2}\\[0.6em]
\frac{\partial^2 g_1}{\partial x_2 \partial x_1} & \frac{\partial^2 g_1}{\partial^2 x_2}\\
\end{pmatrix}
= \begin{pmatrix}
0 & 0\\[0.6em]
0 & 0\\
\end{pmatrix}
\label{eq53}
\end{equation}

Since $H_f(\mathbf{x})$ is a diagonal matrix, its eigenvalues are the elements on its principle diagonal.
Also, since $H_{g_1}(\mathbf{x})$ is a zero matrix, it has only one eigenvalue $0$ of multiplicity $2$.
Since eigenvalues of both matrices are non-negative, both $H_f(\mathbf{x})$ and $H_{g_1}(\mathbf{x})$ are positive semi-definite.
Hence, we have shown both $f$ and $g_1$ are convex functions.

Therefore, according to Kuhn-Tucker theorem, any point $\mathbf{x}$ should satisfy conditions stated in Eq. \ref{eq54} and Eq. \ref{eq55} as well as the active constraint presented in Eq. \ref{eq56} to satisfy the Kuhn-Tucker Sufficient conditions.

\begin{equation}
(\nabla f)_\mathbf{x} + u_1 (\nabla g_1)_\mathbf{x} = \mathbf{0}
\label{eq54}
\end{equation}
\begin{equation}
u_1 \geq 0
\label{eq55}
\end{equation}
\begin{equation}
u_1 g_1(\mathbf{x}) = 0
\label{eq56}
\end{equation}

Given that $f(\mathbf{x}) = x_1^2 + 2x_2^2$ and $g_1(\mathbf{x}) = x_1 + x_2 - 1$, Eq. \ref{eq54} can be rewritten as shown in Eq. \ref{eq57}.
\begin{equation}
\begin{pmatrix} 2x_1 \\ 4x_2 \end{pmatrix} + u_1 \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\label{eq57}
\end{equation}
Therefore,
\begin{equation}
\left\{ \begin{array}{ll} 2x_1+u_1 = 0\\ 4x_2 + u_1 = 0\\ \end{array}\right.
\label{eq58}
\end{equation}
Hence,
\begin{equation}
2(2x_2-x_1) = 0
\label{eq59}
\end{equation}
But at the same time, Eq. \ref{eq56} requires
\begin{equation}
u_1(x_1 + x_2 - 1) = 0
\label{eq60}
\end{equation}
Therefore, either $u_1 = 0$ or $x_1 + x_2 - 1 = 0$.
In case, $u_1 \neq 0$,using Eq. \ref{eq59} $x_1$ and $x_2$ are obtained as $x_1 = 2/3$ and $x_2 = 1/3$.
However, substituting these values in Eq. \ref{eq58} would necessarily result in $u_1 \leq 0$ and this will contradict the Kuhn-Tucker condition in Eq. \ref{eq55}.
Therefore, $u_1 = 0$.
Finally, by substituting $u_1$ in Eq. \ref{eq55}, the point will be obtained as $\mathbf{x}(x_1, x_2) = (0, 0)$.
Since this point adheres to all Kuhn-Tucker conditions, it is an optimal point which minimizes the function $f = x_1^2 + 2x_2^2$ and satisfies $g(x) = -1 \leq 0$.
