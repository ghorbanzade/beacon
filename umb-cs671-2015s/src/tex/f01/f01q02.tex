%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CS671: Machine Learning
% Copyright 2015 Pejman Ghorbanzade <pejman@ghorbanzade.com>
% Creative Commons Attribution-ShareAlike 4.0 International License
% More info: https://github.com/ghorbanzade/beacon
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Question 2}

Consider the set of points in $\mathbb{R}^2$ shown in Figure \ref{fig1}.
Positive examples are shown as black circles, while negative examples are shown as gray circles.
\begin{enumerate}[label=(\alph*)]
\item Find classifiers for the sample using at least four kernels available in the package of $R$ software.
\item Graph the total time required by these kernels as a function of the error rate obtained on the training set.
\end{enumerate}

\begin{figure}[H]\centering
\tikzstyle{black}=[circle, draw, fill=black]
\tikzstyle{white}=[circle, draw, fill=gray!20]
\begin{tikzpicture}
  \draw [thick, color=gray, step=0.5cm, dashed] (0,0) grid (5,5);
  \draw [<->,thick] (0,5.5) node (yaxis) [left] {$y$} |- (5.5,0) node (xaxis) [below] {$x$};
  \foreach \Point in { (4.5,0.5), (4,1), (3.5,1.5), (3,2), (4,2), (2.5,2.5), (3.5,2.5), (3,3), (4,3), (3.5,3.5), (4,4), (4.5,4.5) } {
	\fill[black] \Point circle (3pt);
  }
  \foreach \Point in {(1,0.5), (1,1.5), (1.5,1.5), (1.5,2.5), (1,3), (2,3.5), (2.5,4), (1,4.5), (3, 2.5)} {
    \fill[white] \Point circle (3pt);
  }
\end{tikzpicture}
\caption{Training Sample}\label{fig1}
\end{figure}

\vfill

\subsection*{Solution}

\begin{table}[H]\centering
\begin{tabular}{r l c c}
Kernel & Function Name & Training Error & Execution Time\\
\hline
Linear & \texttt{vanilladot} & 4.761905 & 0.01\\
Radial Basis & \texttt{rbfdot} & 4.761905 & 0.02\\
Polynomial & \texttt{polydot} & 4.761905 & 0.01\\
ANOVA RBF & \texttt{anovadot} & 4.761905 & 0.02\\
Bessel & \texttt{besseldot} & 4.761905 & 0.00\\
Laplacian & \texttt{laplacedot} & 4.761905 & 0.02\\
Spline & \texttt{splinedot} & 9.52381 & 0.01\\
Hyperbolic Tangent & \texttt{tanhdot} & 19.04762 & 0.00\\
\hline
\end{tabular}
\caption{Performance of Different Kernel Functions on Training Sample given in Fig \ref{fig1}}\label{tab1}
\end{table}

The training dataset shown in Figure \ref{fig1} is constructed in comma-separated value format.
Using R programming-language, the training dataset is imported to workspace and is used to construct classifiers using kernels available in the \texttt{kernlab} package.
Table \ref{tab1} provides the list of kernels used for SVM-based classification as well as their training error and execution time for their construction over given training dataset.

Fig \ref{fig2}, generated by R, presents the training error resulting construction of  classifiers using different kernel functions.
As can be seen, \texttt{splinedot} and \texttt{tanhdot} kernels have failed to classify the training dataset accurately.
The similar training error achieved by other kernel functions can be justified by noting the limited amount of samples in the training dataset as well as the way they are formed  which makes it fairly easy to classify.

\begin{figure}\centering
\includegraphics[width=0.8\textwidth]{\pngDirectory/umb-cs671-2015s-f01-02.png}
\caption{Training Error for Different Kernels}\label{fig2}
\end{figure}

Also, plot of execution time of different kernel functions in constructing classifier for the training dataset is given in Figure \ref{fig3}.

\begin{figure}\centering
\includegraphics[width=0.8\textwidth]{\pngDirectory/umb-cs671-2015s-f01-03.png}
\caption{Execution Time for Different Kernel Functions}\label{fig3}
\end{figure}

Figure \ref{fig4} depicts the execution time based on training error for different kernel functions.

\begin{figure}\centering
\includegraphics[width=0.8\textwidth]{\pngDirectory/umb-cs671-2015s-f01-04.png}
\caption{Execution-Time based on Error-Rate for Different Kernels}\label{fig4}
\end{figure}

It can be inferred that using kernel functions that promise better performance in terms of training error might necessitate higher computational costs which lead to longer execution times.
However, this particular example does not prove this point as can be seen in Fig \ref{fig4}.
In fact, rerunning the same script would give different results each time.
This is due to the limited amount of the training dataset that will make the errors in measurement of the execution time quite considerable.
To prove our claim, the same code in R programming language has been applied to the larger database given for letter recognitions, results of which are included in the files attached to this document.

The code in R data-analysis software to construct and compare classifiers over the training dataset given in \ref{fig1} has been given as appendix to this report.
